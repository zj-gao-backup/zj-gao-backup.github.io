@article{ohbm,
    title = {Investigating Task Effects on Brain Activity During Stimulus Presentation in MEG},
    author = {<b>Stretcu, Otilia</b><em class="star">*</em>  and Toneva, Mariya<em class="star">*</em>  and Poczos, Barnabas  and Mitchell, Tom},
    journal = {Human Brain Mapping Conference},
    year = {2019},
    abbr = {HBM},
    abstract = {{Recorded brain activity of subjects who perceive the same stimulus (e.g. a word) while performing
        different semantic tasks (e.g. identifying whether the word belongs to a particular category) has been shown
        to differ across tasks. However, it is not well understood how precisely the task contributes to this brain
        activity. In the current work, we propose multiple hypotheses of how possible interactions between the task and
        stimulus semantics can be related to the observed brain activity. We test these hypotheses by designing machine
        learning models to represent each hypothesis, training them to predict the recorded brain activity, and
        comparing their performance. We show that incorporating task semantics improves the prediction of
        single-trial MEG data by an average of 10% across subjects.}},
}

@inproceedings{platanios2019curriculummt,
    title = {Competence-based Curriculum Learning for Neural Machine Translation},
    author = {Platanios, Emmanouil Antonios and <b>Stretcu, Otilia</b> and Neubig, Graham and Poczos, Barnabas and Mitchell, Tom},
    booktitle = {Proceedings of the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
    year = {2019},
    organization = {Association for Computational Linguistics (ACL)},
    abbr = {NAACL},
    arxiv = {1903.09848},
    abstract = {{Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.}}
}

@article{Chrabaszcz2019SubthalamicNA,
    title = {Subthalamic nucleus and sensorimotor cortex activity during speech production},
    author = {Chrabaszcz, Anna and Neumann,  W. J. and <b>Stretcu, Otilia</b> and Lipski, Witold J. and Bush, Adam and
    Dastolfo-Hromack, Christina and Wang, Dongbin and Crammond, Donald J. and Shaiman, Susan and Walsh Dickey, Michael
    and Holt, Lori L. and Turner, Robert S. and Fiez, Julie A. and Richardson, R. Mark},
    journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
    year = {2019},
    abbr = {JNeurosci},
    website = {https://www.ncbi.nlm.nih.gov/pubmed/30700532},
    abstract = {{The sensorimotor cortex is somatotopically organized to represent the vocal tract articulators, such as lips, tongue, larynx, and jaw. How speech and articulatory features are encoded at the subcortical level, however, remains largely unknown. We analyzed local field potential (LFP) recordings from the subthalamic nucleus (STN) and simultaneous electrocorticography recordings from the sensorimotor cortex of 11 human subjects (1 female) with Parkinson's disease during implantation of deep brain stimulation (DBS) electrodes, while they read aloud three-phoneme words. The initial phonemes involved either articulation primarily with the tongue (coronal consonants) or the lips (labial consonants). We observed significant increases in high gamma (60-150 Hz) power in both the STN and the sensorimotor cortex that began before speech onset and persisted for the duration of speech articulation. As expected from previous reports, in the sensorimotor cortex, the primary articulators involved in the production of the initial consonants were topographically represented by high gamma activity. We found that STN high gamma activity also demonstrated specificity for the primary articulator, although no clear topography was observed. In general, subthalamic high gamma activity varied along the ventral-dorsal trajectory of the electrodes, with greater high gamma power recorded in the dorsal locations of the STN. Interestingly, the majority of significant articulator-discriminative activity in the STN occurred prior to that in sensorimotor cortex. These results demonstrate that articulator-specific speech information is contained within high gamma activity of the STN, but with different spatial and temporal organization compared to similar information encoded in the sensorimotor cortex. Clinical and electrophysiological evidence suggest that the subthalamic nucleus is involved in speech, however, this important basal ganglia node is ignored in current models of speech production. We previously showed that subthalamic nucleus neurons differentially encode early and late aspects of speech production, but no previous studies have examined subthalamic functional organization for speech articulators. Using simultaneous local field potential recordings from the sensorimotor cortex and the subthalamic nucleus in patients with Parkinson's disease undergoing deep brain stimulation surgery, we discovered that subthalamic nucleus high gamma activity tracks speech production at the level of vocal tract articulators, prior to the onset of vocalization and often prior to related cortical encoding.}},
}

@inproceedings{fu2017brainzoom,
    title = {BrainZoom: High Resolution Reconstruction from Multi-modal Brain Signals},
    author = {Fu, Xiao<em class="star">*</em> and Huang, Kejun<em class="star">*</em> and <b>Stretcu, Otilia</b><em class="star">*</em> and Song, Hyun Ah<em class="star">*</em> and Papalexakis, Evangelos and Talukdar, Partha and Mitchell, Tom and Sidiropoulo, Nicholas and Faloutsos, Christos and Poczos, Barnabas},
    booktitle = {Proceedings of the 2017 SIAM International Conference on Data Mining},
    pages = {216--227},
    year = {2017},
    organization = {SIAM},
    abbr = {SDM},
    pdf = {brainzoom/siam_paper.pdf},
    abstract = {{ How close can we zoom in to observe brain activity? Our understanding is limited by the resolution of imaging modalities that exhibit good spatial but poor temporal resolution, or vice-versa. In this paper, we propose BRAINZOOM, an efficient imaging algorithm that cross-leverages multi-modal brain signals. BRAINZOOM (a) constructs high resolution brain images from multi-modal signals, (b) is scalable, and (c) is flexible in that it can easily incorporate various priors on the brain activities, such as sparsity, low rank, or smoothness. We carefully formulate the problem to tackle nonlinearity in the measurements (via variable splitting) and auto-scale between different modal signals, and judiciously design an inexact alternating optimization-based algorithmic framework to handle the problem with provable convergence guarantees. Our experiments using a popular realistic brain signal simulator to generate fMRI and MEG demonstrate that high spatio-temporal resolution brain imaging is possible from these two modalities. The experiments also suggest that smoothness seems to be the best prior, among several we tried. }}
}

@article{zhao2017efficient,
    title = {Efficient Multi-task Feature and Relationship Learning},
    author = {Zhao, Han and <b>Stretcu, Otilia</b> and Negrinho, Renato and Smola, Alex and Gordon, Geoff},
    journal = {In Neural Information Processing Systems Workshop on Learning With Limited Data},
    year = {2017},
    pdf = {fetr/LLD_2017_paper_15.pdf},
    arxiv = {1702.04423},
    abbr = {NIPS},
    abstract = {{ We propose a multi-convex framework for multitask learning that improves pre- dictions by learning relationships both between tasks and between features. Our framework is a generalization of related methods, that either learn task relationships, or feature relationships, but not both. We start with a hierarchical Bayesian model, and use the empirical Bayes method to transform the underlying inference problem into a multi-convex problem. To tackle the multi-convex optimization problem, we propose a block coordinate-wise minimization algorithm that has a closed form solution for each block subproblem. Naively these solutions would be expensive to compute, but by using the theory of doubly stochastic matrices, we are able to reduce the covariance learning subproblem to a minimum-weight perfect matching problem on a complete bipartite graph, and solve it analytically and efficiently. To solve the weight learning subproblem, we propose three different strategies that can be used no matter whether the instances are shared by multiple tasks or not. We demonstrate the efficiency of our method on both synthetic datasets and real-world datasets. Experiments show that the proposed optimization method is orders of magnitude faster than the previous projected gradient method, and our model is able to exploit the correlation structures among multiple tasks and features. }},
    poster = {fetr/poster_fetr_lld.pdf}
}

@article{Stretcu2017UnderstandingTN,
    title = {Understanding the neural basis of speech production using Machine Learning},
    author = {<b>Stretcu, Otilia</b>},
    journal = {Master thesis at Carnegie Mellon University},
    year = {2017},
    website = {https://www.ml.cmu.edu/research/dap-papers/F17/dap-stretcu-otilia.pdf},
    abbr = {CMU},
    abstract = {{<b>Background.</b> Understanding how neurons act together to produce speech is still an open problem. Several studies have attempted to decode different aspects of speech from the cortex neural activity, while a person is speaking [1, 8], but evidence from the medical domain [2] suggests that other brain regions, such as the subthalamic nucleus (STN), may also be involved in speech production. <br/>

                <br/><b>Aim.</b> We explore the problem of decoding properties of speech (e.g. volume, manner, used articulators) from neural activity recordings. From a neuroscience perspective, the goal is to understand
                which properties are encoded in different parts of the cortex and STN. From a machine learning
                (ML) perspective, we are interested discover what models are best at extracting relevant information
                from the scarce and noisy neural activity data.<br/>

                <br/><b>Data</b>. The brain signals are recorded from 14 human subjects, while reading out loud words. The
                data consists of ECoG recordings from the ventral primary motor and primary sensory cortical
                areas, and Local Field Potentials from the STN.<br/>

                <br/><b>Proposed Approach.</b> We approach several decoding tasks: (1) predicting when a person is
                speaking or not, from their neural activity, (2) predicting the manner of speech (e.g. nasal, plosive)
                and what articulators (e.g. tongue, lips) are used, and (3) predicting the volume/loudness of the
                speech. For each of these tasks, we apply a series of ML methods, from simple regression models
                (e.g. ridge regression) to deep learning models (e.g. recurrent neural networks). We apply these
                models on different levels of preprocessing of the data, from electrode signals in time domain to
                particular frequency bands. The goal is to understand which models are able to discover interesting
                patterns, with different levels of domain knowledge required for preprocessing. Finally, we use our
                best models to discover which areas of the brain encode different kinds of information about speech
                production.<br/>

                <br/><b>Results.</b> Our analysis shows that machine learning models are able to discover different speech
                features from the neural activity. We were able to classify when a subject is speaking or not from
                their neural activity in the primary motor and primary sensory cortex with up to 96% accuracy, and
                up to 80% accuracy from the STN (an area whose connection to speech production is not entirely
                understood). We were also able to decode certain features of speech (e.g. voicing, manner) and
                inspect the brain regions and time points that contribute to the prediction. From a ML perspective,
                we observe that even simple models overfit easily on our dataset due to the low-sample, highdimensionality problem, and that parameter tuning and proper regularization methods are crucial
                in making accurate predictions. Finally, we recommend neural network based models if time and
                computation resources are available for tuning the parameters, and simple models otherwise.<br/>

                <br/><b>Broader impacts.</b> Our work has important practical applications in the medical domain. For example, a neural decoder can be used in neuroprosthetics to enable communication for the impaired.
                Moreover, understanding the involvement of the STN in speech can improve the deep brain stimulation techniques used for treating Parkinsonâ€™s disease patients. More generally, our work provides
                an useful overview of which ML models are suitable in dealing with different modalities of brain
                data, which can facilitate further neuroscience studies.<br/>

                <br/><b>Keywords:</b> neuroscience, speech production, ECoG, Local Field Potentials, machine learning.}},
}

@inproceedings{stretcu2015multiple,
    title = {Multiple Frames Matching for Object Discovery in Video},
    author = {<b>Stretcu, Otilia</b> and Leordeanu, Marius},
    booktitle = {Proceedings of the 26th British Machine Vision Conference (BMVC)},
    volume = {1},
    number = {2},
    pages = {3},
    year = {2015},
    abbr = {BMVC},
    pdf = {videoPCA/bmvc_videopca.pdf},
    website = {https://sites.google.com/site/multipleframesmatching/},
    abstract = {{ Automatic discovery of foreground objects in video sequences is an important prob- lem in computer vision with applications to object tracking, video segmentation and clas- sification. We propose an efficient method for the discovery of object bounding boxes and the corresponding soft-segmentation masks across multiple video frames. We offer a graph matching formulation for bounding box selection and refinement using second and higher order terms. Our objective function takes into consideration local, frame-based information, as well as spatiotemporal and appearance consistency over multiple frames. First, we find an initial pool of candidate boxes using a novel and fast foreground esti- mation method in video, based on Principal Component Analysis. Then, we match the boxes across multiple frames using pairwise geometric and appearance terms. Finally, we refine their location and soft-segmentation using higher order potentials that estab- lish appearance regularity over multiple frames. We test our method on the large scale YouTube-Objects dataset and obtain state-of-the-art results on several object classes. }},
    presentation = {http://www.bmva.org/bmvc/2015/papers/paper186/index.html},
    code = {https://drive.google.com/drive/folders/0B7CshFGxfi_5aW13T2h5RDZ3djQ}
}

@article{emim,
    title = {A multi-method driven evaluation of molecular imaging techniques},
    author = {<b>Stretcu, Otilia</b> and Shavit, Yoli and Lio, Pietro},
    journal = {Poster presentation at the 10th annual meeting of the European Society for Molecular Imaging (ESMI)},
    year = {2015},
    abbr = {EMIM},
    organization = {European Society for Molecular Imaging (ESMI)},
    abstract = {{In the past decade, we have witnessed significant advancements in image analysis methods.
        However, the analysis of molecular images such as fluorescent in situ hybridization (FISH) still
        relies heavily on manual evaluation by experts. For automated image analysis to be adopted by
        clinicians there is a need for more reliable tools that can extract rich information from molecular
        images. The first step in developing such tools is to evaluate the performance of state-of-the-art
        image analysis methods. This assessment could be further used to develop of a multi-method
        approach that would integrate partial information extracted by each constituent method, to build an
        augmented microscopic reality.
        We investigate a set of techniques which have shown promising results in the analysis of images
        that share many similarities with molecular images. Namely, wavelets, that have successfully been
        used to describe structural patterns in astronomical images [1], and Markov Random Fields. We
        apply them to a benchmark of molecular FISH images and evaluate their performance with respect
        to two key tasks: (1) denoising and thresholding; (2) segmentation and detection of various
        structures(e.g. interphase nuclei, metaphase chromosomes, multi-colored probes). We further test
        whether data from high-throughput Chromosome Conformation Capture (Hi-C) could enrich the
        information obtained.
        We show the results of our analysis and identify which techniques are best for describing certain
        properties of images (e.g. intensity, edges). We further discuss methodologies of integrating
        complementary methods and data to improve the overall performance.
        The integration of multiple methods can result in overall improvement with respected to key tasks
        in the analysis of molecular images. Coupling a multi-method approach with Chromosome
        Conformation Capture data can help to extract additional information from images.
        <br/><br/>
        [1] Mertens, F., and Lobanov A. (2014) Wavelet-based decomposition and analysis of structural
        patterns in astronomical images. arXiv preprintarXiv:1410.3732}},
    poster = {emim/emim_poster.pdf}
}